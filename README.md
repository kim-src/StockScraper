### ğŸ Tunnel
- [StockScraper ì„¸ë¶€ë‚´ìš© (ê¸€ì œëª© : ì›¹ í¬ë¡¤ë§ì„ ì´ìš©í•œ ë°ì´í„° ë¶„ì„)](https://kim-src.github.io/categories/toy-project/)
- [Kimì˜ ê°œë°œ í”„ë¡œì íŠ¸ ëª¨ìŒ (ê¹ƒí—ˆí”„ ë ˆí¬ì§€í† ë¦¬)](https://github.com/Kim-src/All-Projects)
- [ë§í¬ë“œì¸ í”„ë¡œí•„ (Chang-Seong Kim)](https://www.linkedin.com/in/chang-seong-kim-7826142a0/)

<br>

## âœ… StockScraper ì†Œê°œ
> - ê°œë°œ í™˜ê²½ : Google Colab
> - ê°œë°œ ëª©ì  : Pythonì„ ì´ìš©í•œ ì›¹ í¬ë¡¤ë§ ê²½í—˜ ë° Colab í™œìš© ê²¸í—˜ ìŠµë“
> - ê°œë°œ ë‚´ìš© : ë„¤ì´ë²„ ì¦ê¶Œ ì‚¬ì´íŠ¸ë¥¼ ì´ìš©í•œ ì‚¼ì„±ì „ì ì£¼ì‹ ì •ë³´ ìˆ˜ì§‘
> - ê°œë°œ ì—­ëŸ‰ : Colab í™œìš©, Pythonì˜ BeautifulSoup, Pandas, Requests í™œìš©, ì›¹ í¬ë¡¤ë§ ì´í•´
> - ìƒì„¸ ë‚´ìš© : <a href="https://kim-src.github.io/categories/toy-project/">ì›¹ í¬ë¡¤ë§ì„ ì´ìš©í•œ ë°ì´í„° ë¶„ì„ (ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬)</a>
> - ê°œë°œì— í•„ìš”í–ˆë˜ ê¸°ëŠ¥ :  
> <img alt="Google Colab" src="https://img.shields.io/badge/-Google_Colab-F9AB00?style=flat-square&logo=google-colab&logoColor=white" /> <img alt="Python" src="https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=python&logoColor=white" /> <img alt="BeautifulSoup" src="https://img.shields.io/badge/BeautifulSoup-2ca02c.svg?style=flat-square&logo=python&logoColor=white" /> <img alt="Pandas" src="https://img.shields.io/badge/Pandas-white.svg?style=flat-square&logo=pandas&logoColor=black" /> <img alt="Requests" src="https://img.shields.io/badge/Requests-2CA5E0.svg?style=flat-square&logo=python&logoColor=white" />
> 
> #### ğŸ¯ í•™ìŠµ ê²°ë¡ 
> - ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ í”„ë¡œê·¸ë¨ì„ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤.
> - ì›¹ í¬ë¡¤ë§ì„ ì´ìš©í•˜ì—¬ HTMLë¡œ êµ¬ì„±ëœ ì›¹ í˜ì´ì§€ ì •ë³´ë¥¼ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
> - ë” ë‚˜ì•„ê°€ë©´ ê²€ìƒ‰ ì—”ì§„ í”„ë¡œê·¸ë¨ ê°œë°œì— í™œìš©ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
> - ê·¸ëŸ°ë° ì›¹ í¬ë¡¤ë§ì€ ìˆ˜ë§ì€ ë°ì´í„° ë¶„ì„ ë°©ë²• ì¤‘ ë‹¨ì§€ í•˜ë‚˜ì˜ ë„êµ¬ì¼ë¿ì´ì—ˆìŠµë‹ˆë‹¤.
> - ë¬¼ë¡  ìë™í™” í”„ë¡œê·¸ë¨ ê°œë°œë„ ì¤‘ìš”í•˜ì§€ë§Œ ë°ì´í„° ìˆ˜ì§‘ ê³„íš ë° í•´ì„ ë°©ë²•ì´ ë” ì¤‘ìš”í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<br>

## ğŸ”” ë°°ê²½ ì§€ì‹
### ğŸ“Œ ì›¹ í¬ë¡¤ë§(Web Crawling)
> - ë°ì´í„° ë¶„ì„ ë˜ëŠ” ê²€ìƒ‰ ì—”ì§„ êµ¬ì¶•ì˜ í•œ ë°©ë²•ìœ¼ë¡œ ì›¹ í˜ì´ì§€ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
> - Pyhtonì˜ ë°˜ë³µë¬¸ ë° Requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ë©´ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - ë¸”ë¡œê·¸ ê¸€ì—ì„œëŠ” ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì´ìš©í•œ ì£¼ì‹ ì •ë³´ ë°ì´í„° ë¶„ì„ ë°©ë²•ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

### ğŸ“Œ BeautifulSoup
> - BeautifulSoupëŠ” ëŒ€í‘œì ì¸ ì›¹ í¬ë¡¤ë§ ëª©ì ì˜ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
> - BeautifulSoupëŠ” HTML ë˜ëŠ” XML ë“±ì˜ ì–¸ì–´ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ“Œ Pandas
> - PandasëŠ” ë°ì´í„° ê°€ê³µ ëª©ì ì˜ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
> - ë°ì´í„° ë¶„ì„ ì‹œ ë°ì´í„°ë¥¼ DB í˜•ì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - ë˜ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì™€ ì¡°í•©ì‹œí‚¤ê±°ë‚˜ ë°ì´í„° í•„í„°ë§ ì‹œ ì´ìš©ë©ë‹ˆë‹¤.
> - ê·¸ë˜ì„œ BeautifulSoupë¡œ ë°ì´í„°ë¥¼ ì·¨ë“í•œ í›„ Pandasë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì„ í•˜ê¸°ë„ í•©ë‹ˆë‹¤.

<br>

***

<br>
<br>
<br>



``` python
from bs4 import BeautifulSoup # BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
import requests # Requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

for page in range(1, 6): # í˜ì´ì§€ ìˆ˜ë¥¼ ì˜ë¯¸í•˜ëŠ” 1ë¶€í„° 5ê¹Œì§€ì˜ ìˆ«ìì— ëŒ€í•´ ë°˜ë³µí•˜ì˜€ìŠµë‹ˆë‹¤.

  print(str(page))
  # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
  
  url_005930 = "http://finance.naver.com/item/sise_day.nhn?code=005930" + "&page=" + str(page)
  # ë„¤ì´ë²„ ê¸ˆìœµì˜ ì‚¼ì„±ì „ìì˜ ì£¼ì‹ ì •ë³´ê°€ ìˆëŠ” í˜ì´ì§€ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸(&page=)ë¥¼ str(page) íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.

  headers = {"User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
  # ì›¹ ì„œë²„ê°€ ì›¹ í¬ë¡¤ë§ ìš”ì²­ì„ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì˜¨ ê²ƒìœ¼ë¡œ ì¸ì‹í•˜ê²Œ í•˜ê¸° ìœ„í•´ User-Agent í—¤ë” ì •ë³´ë¥¼ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
  # ì°¸ê³ ë¡œ User-Agent ì •ë³´ëŠ” 'https://useragentstring.com/'ì—ì„œ íšë“í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” ì‚¬ìš©ìì˜ ì›¹ í™˜ê²½ ë“±ì˜ ì •ë³´ì…ë‹ˆë‹¤.

  response = requests.get(url_005930, headers=headers)
  # ì„¤ì •í•œ urlì— HTTP GET ìš”ì²­ì„ ë³´ë‚¸ í›„ response ë³€ìˆ˜ì— í•´ë‹¹ ì‘ë‹µì„ ì €ì¥í•˜ê²Œ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

  soup = BeautifulSoup(response.text, "html.parser")
  # responseì— ì €ì¥ëœ ì‘ë‹µ ì¤‘ HTML ë‚´ìš©ì„ ì‚¬ìš©í•˜ì—¬ BeautifulSoup ê°ì²´ë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
  # BeautifulSoup ê°ì²´ë¥¼ ì´ìš©í•˜ì—¬ HTML ë‚´ìš©ì„ íŒŒì‹±(ë¶„ì„)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  parsing_list = soup.find_all("tr")
  # HTML ë‚´ìš© ì¤‘ 'tr' íƒœê·¸ê°€ í¬í•¨ëœ ëª¨ë“  ë¬¸ìì—´(í–‰)ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ì‹œì¼°ìŠµë‹ˆë‹¤.

  isCheckNone = None
  # None ê°’ì— ëŒ€í•œ ë¹„êµë¥¼ ìœ„í•œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
  
  for i in range(1, len(parsing_list)):
  # parsing_list ë‚´ìš©ì— ìˆëŠ” 'tr' íƒœê·¸ê°€ í¬í•¨ëœ í–‰ì„ ë°˜ë³µí•˜ì˜€ìŠµë‹ˆë‹¤.

    if(parsing_list[i].span != isCheckNone):
    # ìœ íš¨í•œ ë°ì´í„° í–‰ ì •ë³´ë§Œ ì·¨ë“í•˜ê¸° ìœ„í•´ 'span' íƒœê·¸ì˜ ë‚´ìš©ì´ Noneì´ ì•„ë‹Œì§€ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

      print(parsing_list[i].find_all("td", align="center")[0].text,
            # í•´ë‹¹ 'tr' íƒœê·¸ ë‚´ì—ì„œ 'td' íƒœê·¸ ì¤‘ align ì†ì„±ì´ "center"ì¸ ì²« ë²ˆì§¸ ìš”ì†Œ([0])ì˜ í…ìŠ¤íŠ¸(ë‚ ì§œ)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

            parsing_list[i].find_all("td", class_="num")[0].text)
            # ê°™ì€ 'tr' íƒœê·¸ ë‚´ì—ì„œ 'td' íƒœê·¸ ì¤‘ class ì†ì„±ì´ "num"ì¸ ì²« ë²ˆì§¸ ìš”ì†Œ([0])ì˜ í…ìŠ¤íŠ¸(ì£¼ì‹ ì¢…ê°€)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
```
